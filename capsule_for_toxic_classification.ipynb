{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chenpeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import nltk\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\n",
    "from keras.layers import concatenate, GRU, Input, K, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#capsule 的参数\n",
    "\n",
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.3\n",
    "rate_drop_dense = 0.3\n",
    "\n",
    "batch_size = 128 # 256\n",
    "recurrent_units = 16 # 64\n",
    "dropout_rate = 0.3 \n",
    "dense_size = 8 # 32\n",
    "sentences_length = 10 # 300\n",
    "fold_count = 2 # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_path = \"/home/chenpeng/project/Capsule_for_toxic/data/train_small.csv\"\n",
    "test_file_path = \"/home/chenpeng/project/Capsule_for_toxic/data/test_small.csv\"\n",
    "embedding_path = \"/home/chenpeng/project/Capsule_for_toxic/embedding/embeddings_small.vec\"\n",
    "#embedding_path 文件是每个单词（包含字符）对应的300维度的特征  一共100个单词（字符）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据\n"
     ]
    }
   ],
   "source": [
    "UNKNOWN_WORD = \"_UNK_\"\n",
    "END_WORD = \"_END_\"\n",
    "NAN_WORD = \"_NAN_\"\n",
    "CLASSES = [\"project_is_approved\"]\n",
    "\n",
    "print(\"加载数据\")\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "# print(train_data.head())\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "# print(test_data.head())\n",
    "\n",
    "list_sentences_train = train_data[\"application_text\"].fillna(NAN_WORD).values\n",
    "# print(list_sentences_train[:3])#返回的是前三行的信息（一行是一个句子）  以列表的形式\n",
    "list_sentences_test = test_data[\"application_text\"].fillna(NAN_WORD).values\n",
    "\n",
    "y_train = train_data[CLASSES].values#取值0/1 以列表的形式\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, words_dict):\n",
    "    \"\"\"\n",
    "    tokenized_sentences:返回的是每个句子的标注  将每个句子标注化\n",
    "    word_dict:返回的是{单词：索引，单词：索引...}\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        result = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = len(words_dict)#单词词典，{单词：索引}\n",
    "            word_index = words_dict[word]#每个句子的单词索引\n",
    "            result.append(word_index)#获得一个句子的单词索引\n",
    "        tokenized_sentences.append(result)\n",
    "    return tokenized_sentences, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences in train set...\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing sentences in train set...\")\n",
    "tokenized_sentences_train, words_dict = tokenize_sentences(list_sentences_train, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences in test set...\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing sentences in test set...\")\n",
    "tokenized_sentences_test, words_dict = tokenize_sentences(list_sentences_test, words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_dict[UNKNOWN_WORD] = len(words_dict) #向词典中添加词典之外的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embedding_list(file_path):\n",
    "    \"\"\"\n",
    "    file_path:嵌入文件路径\n",
    "    embedding_list:返回的是嵌入单词的向量列表\n",
    "    embedding_word_dict：返回的是嵌入的单词的词典  {单词：索引}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    embedding_word_dict = {}\n",
    "    embedding_list = []\n",
    "    f = open(file_path)\n",
    "\n",
    "    for index, line in enumerate(f):\n",
    "        if index == 0:#第一行跳过\n",
    "            continue\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            continue\n",
    "        embedding_list.append(coefs)\n",
    "        embedding_word_dict[word] = len(embedding_word_dict)\n",
    "    f.close()\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list, embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embeddings...\")\n",
    "embedding_list, embedding_word_dict = read_embedding_list(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_list , embedding_word_dict = read_embedding_list(embedding_path)\n",
    "# print(embedding_list)\n",
    "# print(embedding_word_dict)\n",
    "embedding_size = len(embedding_list[0])#嵌入维度300维度\n",
    "# print(embedding_size)#300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_embedding_list(embedding_list, embedding_word_dict, words_dict):\n",
    "    \"\"\"\n",
    "    words_dict:是训练集的词典\n",
    "    embedding_word_dict：嵌入单词的词典\n",
    "    embedding_list:嵌入单词对应的300维度的向量\n",
    "    \n",
    "    #该函数的作用是清除训练集的词典中的单词不在嵌入词典中 从而根据嵌入词典得到对应的嵌入向量列表\n",
    "    #即根据嵌入向量  得到对应的训练集的嵌入\n",
    "    \"\"\"\n",
    "    cleared_embedding_list = []\n",
    "    cleared_embedding_word_dict = {}\n",
    "\n",
    "    for word in words_dict:\n",
    "        if word not in embedding_word_dict:#若果训练集的词典不在嵌入词典中 则 跳过当前操作\n",
    "            continue\n",
    "        word_id = embedding_word_dict[word]\n",
    "        row = embedding_list[word_id]\n",
    "        cleared_embedding_list.append(row)\n",
    "        cleared_embedding_word_dict[word] = len(cleared_embedding_word_dict)\n",
    "\n",
    "    return cleared_embedding_list, cleared_embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "(79, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data...\")\n",
    "embedding_list, embedding_word_dict = clear_embedding_list(embedding_list, embedding_word_dict, words_dict)\n",
    "\n",
    "embedding_word_dict[UNKNOWN_WORD] = len(embedding_word_dict)#向嵌入词典中添加 UNKNOWN_WORD\n",
    "embedding_list.append([0.] * embedding_size)#UNKNOWN_WORD 对应的嵌入向量 为0 \n",
    "embedding_word_dict[END_WORD] = len(embedding_word_dict)##向嵌入词典中添加 END_WORD\n",
    "embedding_list.append([-1.] * embedding_size)#END_WORD 对应的为-1\n",
    "\n",
    "embedding_matrix = np.array(embedding_list)#得到嵌入矩阵\n",
    "print(np.shape(embedding_matrix))#embedding_matrix :80 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_to_word = dict((id, word) for word, id in words_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(tokenized_sentences, words_list, embedding_word_dict, sentences_length):\n",
    "    \"\"\"\n",
    "    tokenized_sentences:标注化的句子列表\n",
    "    words_list：索引到单词 （根据训练集的dic）\n",
    "    embedding_word_dict:嵌入矩阵的词典\n",
    "    sentences_length:句子的长度 设为10\n",
    "    \"\"\"\n",
    "    words_train = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        current_words = []\n",
    "        for word_index in sentence:\n",
    "            word = words_list[word_index]\n",
    "            word_id = embedding_word_dict.get(word, len(embedding_word_dict) - 2)\n",
    "            current_words.append(word_id)\n",
    "\n",
    "        if len(current_words) >= sentences_length:\n",
    "            current_words = current_words[:sentences_length]\n",
    "        else:\n",
    "            current_words += [len(embedding_word_dict) - 1] * (sentences_length - len(current_words))\n",
    "        words_train.append(current_words)\n",
    "    return words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_of_token_ids = convert_tokens_to_ids(\n",
    "    tokenized_sentences_train,\n",
    "    id_to_word,\n",
    "    embedding_word_dict,\n",
    "    sentences_length)\n",
    "\n",
    "test_list_of_token_ids = convert_tokens_to_ids(\n",
    "    tokenized_sentences_test,\n",
    "    id_to_word,\n",
    "    embedding_word_dict,\n",
    "    sentences_length)\n",
    "\n",
    "X_train = np.array(train_list_of_token_ids)\n",
    "X_test = np.array(test_list_of_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "capsuleLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"获取模型\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, sequence_length, dropout_rate, recurrent_units, dense_size):\n",
    "    input1 = Input(shape=(sequence_length,))\n",
    "    embed_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(1, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 300)           23700     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 256)           329472    \n",
      "_________________________________________________________________\n",
      "capsule_1 (Capsule)          (None, 10, 16)            40960     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 394,293\n",
      "Trainable params: 370,593\n",
      "Non-trainable params: 23,700\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fe0d00fd550>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model(embedding_matrix,\n",
    "    sentences_length,\n",
    "    dropout_rate,\n",
    "    recurrent_units,\n",
    "    dense_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_model_func = lambda: get_model(\n",
    "    embedding_matrix,\n",
    "    sentences_length,\n",
    "    dropout_rate,\n",
    "    recurrent_units,\n",
    "    dense_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7fe0d8bbf950>\n"
     ]
    }
   ],
   "source": [
    "print(get_model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train_model(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    num_labels = train_y.shape[1]\n",
    "    patience = 5\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    current_epoch = 0\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epochs=1)\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "\n",
    "        total_loss = 0\n",
    "        for j in range(num_labels):\n",
    "            loss = log_loss(val_y[:, j], y_pred[:, j])\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_labels\n",
    "\n",
    "        print(\"Epoch {0} loss {1} best_loss {2}\".format(current_epoch, total_loss, best_loss))\n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss < best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == patience:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_folds(X, y, X_test, fold_count, batch_size, get_model_func):\n",
    "    print(\"=\"*75)\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    result_path = \"predictions\"\n",
    "    if not os.path.exists(result_path):\n",
    "        os.mkdir(result_path)\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = np.array(X[fold_start:fold_end])\n",
    "        val_y = np.array(y[fold_start:fold_end])\n",
    "\n",
    "        model = _train_model(get_model_func(), batch_size, train_x, train_y, val_x, val_y)\n",
    "        train_predicts_path = os.path.join(result_path, \"train_predicts{0}.npy\".format(fold_id))\n",
    "        test_predicts_path = os.path.join(result_path, \"test_predicts{0}.npy\".format(fold_id))\n",
    "        train_predicts = model.predict(X, batch_size=512, verbose=1)\n",
    "        test_predicts = model.predict(X_test, batch_size=512, verbose=1)\n",
    "        np.save(train_predicts_path, train_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 10, 300)           23700     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 10, 256)           329472    \n",
      "_________________________________________________________________\n",
      "capsule_4 (Capsule)          (None, 10, 16)            40960     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 394,293\n",
      "Trainable params: 370,593\n",
      "Non-trainable params: 23,700\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 2s 38ms/step - loss: 0.5367 - acc: 0.8200\n",
      "Epoch 0 loss 0.42185745525116825 best_loss -1\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 649us/step - loss: 0.4457 - acc: 0.8400\n",
      "Epoch 1 loss 0.42015412039294536 best_loss 0.42185745525116825\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 552us/step - loss: 0.4233 - acc: 0.8400\n",
      "Epoch 2 loss 0.4322174141297535 best_loss 0.42015412039294536\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 553us/step - loss: 0.4734 - acc: 0.8400\n",
      "Epoch 3 loss 0.43717909946429484 best_loss 0.42015412039294536\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 585us/step - loss: 0.4503 - acc: 0.8400\n",
      "Epoch 4 loss 0.4342079056921054 best_loss 0.42015412039294536\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 564us/step - loss: 0.4550 - acc: 0.8400\n",
      "Epoch 5 loss 0.4271425455048376 best_loss 0.42015412039294536\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 545us/step - loss: 0.4648 - acc: 0.8400\n",
      "Epoch 6 loss 0.4184380482350077 best_loss 0.42015412039294536\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 585us/step - loss: 0.4554 - acc: 0.8400\n",
      "Epoch 7 loss 0.4101019846845646 best_loss 0.4184380482350077\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 554us/step - loss: 0.4400 - acc: 0.8400\n",
      "Epoch 8 loss 0.40404525551260734 best_loss 0.4101019846845646\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 586us/step - loss: 0.4148 - acc: 0.8400\n",
      "Epoch 9 loss 0.40154695890995923 best_loss 0.40404525551260734\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 580us/step - loss: 0.4042 - acc: 0.8400\n",
      "Epoch 10 loss 0.4027460152093245 best_loss 0.40154695890995923\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 596us/step - loss: 0.4035 - acc: 0.8400\n",
      "Epoch 11 loss 0.40709078889720296 best_loss 0.40154695890995923\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 557us/step - loss: 0.3960 - acc: 0.8400\n",
      "Epoch 12 loss 0.4127741196021742 best_loss 0.40154695890995923\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 533us/step - loss: 0.4255 - acc: 0.8400\n",
      "Epoch 13 loss 0.41722071353270085 best_loss 0.40154695890995923\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 567us/step - loss: 0.3798 - acc: 0.8400\n",
      "Epoch 14 loss 0.41929238046310385 best_loss 0.40154695890995923\n",
      "99/99 [==============================] - 0s 192us/step\n",
      "99/99 [==============================] - 0s 158us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 10, 300)           23700     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 10, 256)           329472    \n",
      "_________________________________________________________________\n",
      "capsule_5 (Capsule)          (None, 10, 16)            40960     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 394,293\n",
      "Trainable params: 370,593\n",
      "Non-trainable params: 23,700\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.8940 - acc: 0.1800\n",
      "Epoch 0 loss 0.5127308654541872 best_loss -1\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 662us/step - loss: 0.5583 - acc: 0.8200\n",
      "Epoch 1 loss 0.42903549938785784 best_loss 0.5127308654541872\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 559us/step - loss: 0.4257 - acc: 0.8600\n",
      "Epoch 2 loss 0.4478853154851466 best_loss 0.42903549938785784\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 570us/step - loss: 0.3955 - acc: 0.8600\n",
      "Epoch 3 loss 0.47795592978292584 best_loss 0.42903549938785784\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 606us/step - loss: 0.4204 - acc: 0.8600\n",
      "Epoch 4 loss 0.4995278240162499 best_loss 0.42903549938785784\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 562us/step - loss: 0.4123 - acc: 0.8600\n",
      "Epoch 5 loss 0.5123640783891386 best_loss 0.42903549938785784\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 0s 549us/step - loss: 0.4273 - acc: 0.8600\n",
      "Epoch 6 loss 0.5185519513883152 best_loss 0.42903549938785784\n",
      "99/99 [==============================] - 0s 181us/step\n",
      "99/99 [==============================] - 0s 152us/step\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "models = train_folds(X_train, y_train, X_test, fold_count, batch_size, get_model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
